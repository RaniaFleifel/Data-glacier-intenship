{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd;\n",
    "import os\n",
    "path='D:/RaniaTarek_JobJourney/Courses content/Data glacier internship/submitted/Data science project_Bank Marketing Campaign/'\n",
    "files=['bank-additional-full.csv','bank-full.csv','bank-additional.csv','bank.csv']\n",
    "\n",
    "dfs=[]\n",
    "for file_name in files:\n",
    "    \n",
    "    file=path+file_name\n",
    "    \n",
    "    df_current=pd.read_csv(file,delimiter=';');\n",
    "    df_current['filename']=file_name.split()\n",
    "    num_columns=len(df_current.columns.tolist())\n",
    "    column_names=df_current.columns.tolist()\n",
    "    print('\\nInformation about',file_name)\n",
    "\n",
    "    print('Number of columns= {} '.format(num_columns));\n",
    "\n",
    "    print('Size of file= {} KB'.format(round(os.stat(file).st_size/1024),2));\n",
    "\n",
    "    print('Number of duplicated entries= {}'.format(sum(df_current.duplicated())));\n",
    "    if sum(df_current.duplicated()!=0):\n",
    "        print('{} duplicates found and deleted'.format(sum(df_current.duplicated())))\n",
    "        df_current=df_current.drop_duplicates()\n",
    "\n",
    "    dfs.append(df_current)\n",
    "    print('Number of unique entries= {}'.format(df_current.size/num_columns));\n",
    "\n",
    "    \n",
    "    print('Columns names are {}\\n##############################################################'.format(column_names));\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import calendar\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder,OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import datetime\n",
    "df=pd.read_csv('dff_old.csv')\n",
    "\n",
    "#IMPUTING economic indicators \n",
    "\n",
    "xls = pd.ExcelFile('toappend2.xlsx')\n",
    "print('Sheet names are',xls.sheet_names)\n",
    "\n",
    "# to read just one sheet to dataframe:\n",
    "df30_ = pd.read_excel('toappend2.xlsx', sheet_name=\"daily\")\n",
    "#display(df30_)\n",
    "\n",
    "df30_[\"day\"] = df30_['Date'].map(lambda x: x.day)\n",
    "df30_[\"month\"] = df30_['Date'].map(lambda x: x.month)\n",
    "df30_[\"month\"] = df30_[\"month\"].apply(lambda x: calendar.month_abbr[x].lower())\n",
    "df30_[\"year\"] = df30_['Date'].map(lambda x: x.year)\n",
    "df30= df30_.iloc[::-1].reset_index(drop=True)\n",
    "\n",
    "df['euribor3m'] = 0.0\n",
    "\n",
    "for i in range(len(df30)):    \n",
    "    i_raw=df30.iloc[i,:]\n",
    "    df.loc[(df.day==i_raw.day) & (df.month==i_raw.month) & (df.year==i_raw.year.astype(float)),'euribor3m']=i_raw.Value\n",
    "\n",
    "df['euribor3m_ffill']=0\n",
    "\n",
    "for c in ['euribor3m']:\n",
    "    idx=df[df[c]==float(0)].index\n",
    "    df.loc[idx,c]=None\n",
    "    df.loc[idx,'euribor3m_ffill']=1\n",
    "    display('prefilling',df.loc[idx,c])\n",
    "df.ffill(axis = 0,inplace=True)\n",
    "\n",
    "locs=df.loc[df[\"euribor3m\"].isna()]\n",
    "grp_locs=locs.groupby(['year','month','day']).count()\n",
    "display('rest',grp_locs)\n",
    "for l in range(len(grp_locs)):\n",
    "    d=0\n",
    "    print(l)\n",
    "    dayy=locs.loc[l,'day']\n",
    "    yearr=locs.loc[l,'year']\n",
    "    monthh=locs.loc[l,'month'].capitalize()\n",
    "    m=list(calendar.month_abbr).index(monthh)\n",
    "    orig_d=datetime.datetime(int(yearr),m,dayy)\n",
    "    print('orid_d',orig_d)\n",
    "    d=orig_d\n",
    "    #print(d)\n",
    "    d-= datetime.timedelta(days=1)\n",
    "    #print(d)\n",
    "    found=0\n",
    "    while(found==0):\n",
    "        if df30.query(\"Date==@d\").empty:\n",
    "            d-=datetime.timedelta(days=1)\n",
    "            print('up')\n",
    "        else:\n",
    "            val=df30.query(\"Date==@d\")['Value']\n",
    "            month_lower=monthh.lower()\n",
    "\n",
    "            #display(df.query(\"year==@yearr & month==@month_lower & day==@dayy\")['euribor3m'])\n",
    "            \n",
    "            print('fill date {} from {} by value {}'.format(orig_d,d,val.values[0]))\n",
    "            tofill=df.query(\"year==@yearr & month==@month_lower & day==@dayy\").index.tolist()\n",
    "            print('indicestofill',tofill)\n",
    "            df.loc[tofill,'euribor3m']=val.values[0]\n",
    "            display(df.query(\"year==@yearr & month==@month_lower & day==@dayy\")['euribor3m'])\n",
    "\n",
    "\n",
    "            \n",
    "            found=1\n",
    "            \n",
    "df12_ = pd.read_excel('toappend2.xlsx', sheet_name=\"monthly\")\n",
    "#Harmonised index of consumer prices, Consumer confidence indicator\n",
    "\n",
    "df12_.columns\n",
    "df12_.rename(columns={df12_.columns[0]: 'Date', df12_.columns[1]: 'consum_prices_rate',df12_.columns[2]: 'consum_conf_ind'},inplace=True)\n",
    "df12_=df12_.iloc[3:]\n",
    "\n",
    "df12_[\"day\"] = df12_['Date'].map(lambda x: x.day)\n",
    "df12_[\"month\"] = df12_['Date'].map(lambda x: x.month)\n",
    "df12_[\"month\"] = df12_[\"month\"].apply(lambda x: calendar.month_abbr[x].lower())\n",
    "df12_[\"year\"] = df12_['Date'].map(lambda x: x.year)\n",
    "df12= df12_.iloc[::-1].reset_index(drop=True)\n",
    "\n",
    "#df12.head()\n",
    "df['consum_prices_rate'] = 0.0\n",
    "df['consum_conf_ind'] = 0.0\n",
    "\n",
    "for i in range(len(df12)):    \n",
    "    i_raw=df12.iloc[i,:]\n",
    "    #print(i,i_raw.Value)\n",
    "    df.loc[(df.day<=i_raw.day) & (df.month==i_raw.month) & (df.year==i_raw.year.astype(float)),'consum_prices_rate']=i_raw.consum_prices_rate\n",
    "    df.loc[(df.day<=i_raw.day) & (df.month==i_raw.month) & (df.year==i_raw.year.astype(float)),'consum_conf_ind']=i_raw.consum_conf_ind    \n",
    "\n",
    "df4_ = pd.read_excel('toappend2.xlsx', sheet_name=\"quaretrly\")\n",
    "\n",
    "df4_.columns\n",
    "df4_.rename(columns={df4_.columns[0]: 'Date', df4_.columns[1]: 'employed',df4_.columns[2]: 'unemployed',df4_.columns[3]: 'unemployed_rate'},inplace=True)\n",
    "df4_=df4_.iloc[3:]\n",
    "\n",
    "df4_[\"month\"] = df4_['Date'].map(lambda x: x.month)\n",
    "df4_[\"month\"] = df4_[\"month\"].apply(lambda x: calendar.month_abbr[x].lower())\n",
    "df4_[\"year\"] = df4_['Date'].map(lambda x: x.year)\n",
    "df4= df4_.iloc[::-1].reset_index(drop=True)\n",
    "\n",
    "#display(df4.head())\n",
    "df['employed'] = 0.0\n",
    "df['unemployed'] = 0.0\n",
    "df['unemployed_rate']=0.0\n",
    "\n",
    "for i in range(len(df4)):    \n",
    "    i_raw=df4.iloc[i,:]\n",
    "    #print(i,i_raw.Value)\n",
    "    df.loc[(df.month<=i_raw.month) & (df.year==i_raw.year.astype(float)),'employed']=i_raw.employed\n",
    "    df.loc[(df.month<=i_raw.month) & (df.year==i_raw.year.astype(float)),'unemployed']=i_raw.unemployed\n",
    "    df.loc[(df.month<=i_raw.month) & (df.year==i_raw.year.astype(float)),'unemployed_rate']=i_raw.unemployed_rate    \n",
    "\n",
    "df.to_csv(\"after_append.csv\",\n",
    "          index=False,\n",
    "          encoding=\"utf-8\")\n",
    "\n",
    "# Ambiguous number of entries per feature:\n",
    "# 288 unknowsn in job, 1857 unknowsn in education, 13020 unkowns in contact , 1840 other in poutcome, 36959 unkowns in poutcome\n",
    "\n",
    "# poutcome \n",
    "ct_table_ind=pd.crosstab(df[\"poutcome\"],df[\"pdays\"]==999)\n",
    "print('contingency_table between poutcome and pdays==999:\\n{}'.format(ct_table_ind))\n",
    "#change unknown to nonexistant where pdays==999, ow delete row\n",
    "df.loc[((df['poutcome']=='unknown') & (df[\"pdays\"] == 999)), \"poutcome\"] = 'non_existant'\n",
    "indices_to_drop=df.loc[(df['poutcome']==\"unknown\") & (df['pdays']!=999)].index\n",
    "df.drop(indices_to_drop,axis=0,inplace=True)\n",
    "df.reset_index(inplace=True)\n",
    "#change pdays=999 to pdays=0 to avoid contributing to outliers \n",
    "df.loc[df[\"pdays\"] == 999, \"pdays\"] = 0\n",
    "df.drop('index',axis=1,inplace=True)\n",
    "#df\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for c in ['job','marital']:\n",
    "    values=pd.DataFrame()\n",
    "    norm=pd.DataFrame()\n",
    "\n",
    "    values['yes']=df[df['y']=='yes'][c].value_counts().sort_values()\n",
    "    values['no']=df[df['y']=='no'][c].value_counts().sort_values()\n",
    "    display(values)\n",
    "    norm['yes']=values['yes']/values.max(axis=1)\n",
    "    norm['no']=values['no']/values.max(axis=1)\n",
    "\n",
    "    display(norm)\n",
    "\n",
    "\n",
    "    norm.plot(kind='bar')\n",
    "    plt.xlabel(f'{c}')\n",
    "    plt.ylabel('Number of clients')\n",
    "    plt.title('{}'.format(c))\n",
    "    plt.show()\n",
    "    \n",
    "df.groupby(['job','education'])['age'].count()#*100/df.query('age<21').groupby(['job'])['age'].count()\n",
    "\n",
    "df.loc[df.y==\"yes\",'y_equiv']=int(1)\n",
    "df.loc[df.y==\"no\",'y_equiv']=int(0)\n",
    "\n",
    "#flags for \"unknown values\" in https://machinelearningmastery.com/binary-flags-for-missing-values-for-machine-learning/\n",
    "\n",
    "df['contact_missing'] = 0\n",
    "df['poutcome_missing'] = 0\n",
    "df['job_missing'] = 0\n",
    "df['education_missing'] = 0\n",
    "\n",
    "df.loc[df.contact==\"unknown\",'contact_missing']=1\n",
    "df.loc[df.poutcome==\"other\",'poutcome_missing']=1\n",
    "df.loc[df.job==\"unknown\",'job_missing']=1\n",
    "df.loc[df.education==\"unknown\",'education_missing']=1\n",
    "\n",
    "#nominal and cardinal \n",
    "#https://pianalytix.com/how-do-you-handle-missing-values-categorical-data-and-feature-scaling-in-machine-learning/\n",
    "#sefidian.com/2021/07/02/measure-the-correlation-between-numerical-and-categorical-variables-and-the-correlation-between-two-categorical-variables-in-python-chi-square-and-anova/    \n",
    "\n",
    "####################################################### balance \n",
    "df.loc[df[\"balance\"]<0,'overdraft']=1 #this indicates bad credit score so overall not the best candidate to waste time on\n",
    "df.loc[df[\"balance\"]>=0,'overdraft']=0\n",
    "\n",
    "df.loc[df[\"balance\"]<0,'balance']=0 #as if balancerepresent +ve balance only\n",
    "#binning of balance then label_encode\n",
    "#assume5bgroups\n",
    "bins_it=(df.balance.max()-df.balance.min())/5\n",
    "bins=[]#[df.balance.min()]\n",
    "for i in range(1,5):\n",
    "    bins.append(bins_it*i+1)\n",
    "\n",
    "bins.append(df.balance.max())\n",
    "#bins=[bins_middle,df.balance.max()]\n",
    "print('bins based on train data',bins)\n",
    "orig_balance=df['balance']\n",
    "df['balance'] = pd.cut(df['balance'], bins=5, labels=['vlow','low','med','high','vhigh'])\n",
    "\n",
    "#make all categoric features numeric ! \n",
    "#cats\n",
    "#job --> hot encoding, marital as well and education as well ? \n",
    "#default,housing,load,contact yes=1,no=0, contact=unknown==0(i have another repreesntative)\n",
    "#month -> numerical\n",
    "#day_of_week -> \n",
    "#fix ranges where numeric variables exist! \n",
    "\n",
    "binary=['default','housing','loan','contact','poutcome']\n",
    "orig_binary=df[binary]\n",
    "\n",
    "df.loc[df['default']==\"yes\",'default']=1\n",
    "df.loc[df['default']==\"no\",'default']=0\n",
    "\n",
    "df.loc[df['housing']==\"yes\",'housing']=1\n",
    "df.loc[df['housing']==\"no\",'housing']=0\n",
    "\n",
    "df.loc[df['loan']==\"yes\",'loan']=1\n",
    "df.loc[df['loan']==\"no\",'loan']=0\n",
    "\n",
    "df.loc[df['contact']==\"cellular\",'contact']=1\n",
    "df.loc[(df['contact'] == \"telephone\") | (df['contact'] == \"unknown\"),'contact']=0 #we can cast unknowns as 0 now that we have a variable to indicate it ! \n",
    "\n",
    "df.loc[df['poutcome'] ==\"success\",'poutcome']=1\n",
    "df.loc[(df['poutcome'] == \"non_existant\") | (df['poutcome'] == \"failure\") | (df['poutcome'] == \"other\"),'poutcome']=0\n",
    "\n",
    "\n",
    "months = {month: index for index, month in enumerate(calendar.month_abbr) if month}\n",
    "months['Jan']\n",
    "for month in months:\n",
    "    #print(month.lower(),months[month])\n",
    "    df.loc[df['month']==month.lower(),'month']=months[month]\n",
    "\n",
    "df.head(5)\n",
    "\n",
    "df.to_csv(\"before_hotencoding.csv\",\n",
    "          index=False,\n",
    "          encoding=\"utf-8\")\n",
    "\n",
    "df['balance']=df.balance.cat.codes\n",
    "\n",
    "#for high cardinality job according to how frequent a value appears, encode it .. BEFORE THIS STEP, SPLIT DATA FOR TRAINING AND TESTING AND ONLY USE TARGETS OF TRAINING DATA\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[df.columns[df.columns!='y_equiv']], df['y_equiv'], test_size=0.2, random_state=11)\n",
    "\n",
    "#https://analyticsindiamag.com/a-complete-guide-to-categorical-data-encoding/\n",
    "#https://towardsdatascience.com/dealing-with-categorical-variables-by-using-target-encoder-a0f1733a4c69\n",
    "categories = X_train.job.unique()#df['job'].unique()\n",
    "targets = y_train.astype('int32').unique()#df['y_equiv'].astype('int32').unique()\n",
    "cat_list = []\n",
    "for cat in categories:\n",
    "    aux_dict = {}\n",
    "    aux_dict['category'] = cat\n",
    "    aux_df = X_train[X_train['job'] == cat]\n",
    "    counts = df.loc[X_train[X_train['job'] == cat].index,'y_equiv'].value_counts()\n",
    "    aux_dict['count'] = sum(counts)\n",
    "    for t in targets:\n",
    "# #        print(t)\n",
    "        aux_dict['target_' + str(t)] = counts[t]\n",
    "    cat_list.append(aux_dict)\n",
    "cat_list = pd.DataFrame(cat_list)\n",
    "cat_list['genre_encoded_dumb'] = cat_list['target_1'] / cat_list['count']\n",
    "display(cat_list)\n",
    "orig_job=cat_list\n",
    "\n",
    "for cat in cat_list.category:\n",
    "    educ_equiv=cat_list.loc[cat_list['category']==cat,'genre_encoded_dumb'].tolist()[0]\n",
    "    #print(cat,educ_equiv)\n",
    "    df.loc[df['job']==cat,'job']=educ_equiv#.astype('float64')\n",
    "\n",
    "cat_list.to_csv(\"education_list.csv\",\n",
    "          index=False,\n",
    "          encoding=\"utf-8\")\n",
    "\n",
    "# pd.DataFrame(X_train,X_test)\n",
    "#y_train\n",
    "#y_test\n",
    "train_set=X_train.join(y_train)\n",
    "train_set.to_csv(\"train_set.csv\", index=True, index_label='idx',encoding=\"utf-8\")\n",
    "\n",
    "test_set=X_test.join(y_test)\n",
    "test_set.to_csv(\"test_set.csv\", index=True, index_label='idx', encoding=\"utf-8\")\n",
    "\n",
    "# #ordinal encoding for education\n",
    "orig_education=df['education']\n",
    "\n",
    "ordinal_educ=df.education.unique().tolist()\n",
    "ordinal_educ_encoded=[3,2,0,1]\n",
    "display(ordinal_educ)\n",
    " \n",
    "#for col in [\"education\"]:\n",
    "#    df[col]=LabelEncoder().fit_transform(df[col])\n",
    "#    print(LabelEncoder().fit(df[col]).classes_)\n",
    "#label encoder assigns ['tertiary', 'secondary', 'unknown', 'primary'] as [0:3] hence losing the ordinal idea \n",
    "ind=0\n",
    "for e in ordinal_educ:\n",
    "    df.loc[df['education']==e,'education']=ordinal_educ_encoded[ind]\n",
    "    ind=ind+1\n",
    "    \n",
    "orig_mairal=df.marital\n",
    "dum_df = pd.get_dummies(df['marital'], columns=[\"marital\"] )\n",
    "# merge with main df bridge_df on key values\n",
    "df = df.join(dum_df)\n",
    "df.drop(['marital'],axis=1,inplace=True)\n",
    "\n",
    "df.to_csv(\"before_hotencoding2.csv\",\n",
    "          index=False,\n",
    "          encoding=\"utf-8\")\n",
    "\n",
    "#nominal data hot encoding\n",
    "#for col in [\"day_of_week\",\"marital\"]:\n",
    "orig_dates=df[[\"day_of_week\",\"year\",\"month\"]]\n",
    "for col in [\"day_of_week\",\"year\",\"month\"]:    \n",
    "    #str(col+'_encoded')\n",
    "    days_enc=pd.DataFrame(OneHotEncoder().fit_transform(df[[col]]).toarray())\n",
    "    colname=[]\n",
    "    for day in range(len(days_enc.columns)):#.column:\n",
    "        #print(day)\n",
    "        colname.append(str(col)+str(day+1)) \n",
    "    \n",
    "    #display(colname)\n",
    "    df[colname] = days_enc\n",
    "\n",
    "df.columns\n",
    "#orig #df.drop(['day_of_week','year','month','day'],axis=1,inplace=True)\n",
    "df.drop(['day_of_week'],axis=1,inplace=True)\n",
    "\n",
    "df.head(10)\n",
    "df.to_csv(\"week9.csv\",\n",
    "          index=False,\n",
    "          encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-9d1390c3b5c6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'before_hotencoding2.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\pandas\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;31m# numpy compat\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m from pandas.compat import (\n\u001b[0m\u001b[0;32m     23\u001b[0m     \u001b[0mnp_version_under1p18\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_np_version_under1p18\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[0mis_numpy_dev\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_is_numpy_dev\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\pandas\\compat\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_typing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m from pandas.compat.numpy import (\n\u001b[0m\u001b[0;32m     16\u001b[0m     \u001b[0mis_numpy_dev\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[0mnp_array_datetime64_compat\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\pandas\\compat\\numpy\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mversion\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mVersion\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m# numpy versioning\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\pandas\\util\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m from pandas.util._decorators import (  # noqa\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[0mAppender\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mSubstitution\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mcache_readonly\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m )\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_libs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mproperties\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcache_readonly\u001b[0m  \u001b[1;31m# noqa\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_typing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\pandas\\_libs\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_libs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minterval\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mInterval\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m from pandas._libs.tslibs import (\n\u001b[0;32m     15\u001b[0m     \u001b[0mNaT\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_find_spec\u001b[1;34m(name, path, target)\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\importlib\\_bootstrap_external.py\u001b[0m in \u001b[0;36mfind_spec\u001b[1;34m(cls, fullname, path, target)\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\importlib\\_bootstrap_external.py\u001b[0m in \u001b[0;36m_get_spec\u001b[1;34m(cls, fullname, path, target)\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\importlib\\_bootstrap_external.py\u001b[0m in \u001b[0;36mfind_spec\u001b[1;34m(self, fullname, target)\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\importlib\\_bootstrap_external.py\u001b[0m in \u001b[0;36m_fill_cache\u001b[1;34m(self)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "df=pd.read_csv('before_hotencoding2.csv')\n",
    "dff=pd.read_csv('week9.csv')\n",
    "display(df.columns)\n",
    "#df.describe()\n",
    "# check the corr with the variables ['age','duration','pdays','campaign','previous']:\n",
    "#df.corr()\n",
    "\n",
    "\n",
    "### Personal standing financially: \n",
    "\n",
    "personal_info=['age','job','education','balance','overdraft','default', 'housing', 'loan','divorced','married','single']#,'y_equiv']\n",
    "\n",
    "\n",
    "#########################################################################\n",
    "indebt=df['default']+df['housing']+df['loan']\n",
    "df['indebt']=indebt\n",
    "personal_info.append('indebt')\n",
    "######################################################################3\n",
    "df.loc[df['age']>65,'age_grp']='old'\n",
    "df.loc[df['age']<=25,'age_grp']='young'\n",
    "df.loc[(df['age']<=65) & (df['age']>25),'age_grp']='adult'\n",
    "\n",
    "orig_age_grp=df['age_grp']\n",
    "\n",
    "ordinal_age_grp=df.age_grp.unique().tolist()\n",
    "ordinal_age_grp_encoded=[1,0,2]\n",
    "#display(ordinal_age_grp)\n",
    " \n",
    "ind=0\n",
    "for e in ordinal_age_grp:\n",
    "    df.loc[df['age_grp']==e,'age_grp']=int(ordinal_age_grp_encoded[ind])\n",
    "    ind=ind+1\n",
    "df['age_grp']=df['age_grp'].astype('int64')\n",
    "\n",
    "personal_info.append('age_grp')\n",
    "\n",
    "personal_info=['age','age_grp','job','education','balance','overdraft','default', 'housing', 'loan','indebt','divorced','married','single']#,'y_equiv']\n",
    "df[personal_info].info()\n",
    "\n",
    "\n",
    "for col in df:\n",
    "    if df[col].dtype=='O': \n",
    "        print(\"\\n{} values--> {}\".format(col,set(df[col])));\n",
    "    else:\n",
    "        print(\"\\n{} min={} , max={} \".format(col,df[col].min(),df[col].max()));\n",
    "        \n",
    "educ_transform=pd.read_csv('education_list.csv')\n",
    "\n",
    "##hypothesis 1\n",
    "df.groupby(['age_grp'])[['age','overdraft']].sum()\n",
    "\n",
    "##hypothesis 2\n",
    "df.groupby(['indebt','age_grp'])[['housing']].count()*100/df.groupby(['age_grp'])[['housing']].count()\n",
    "\n",
    "##hypothesis 3\n",
    "df.groupby(['education','balance'])['age'].count()*100/df.groupby(['balance'])['age'].count()\n",
    "df.groupby(['age_grp','balance'])['age'].count()*100/df.groupby(['balance'])['age'].count()\n",
    "\n",
    "##hypothesis 4\n",
    "hyp_4=df.query('age_grp==2').groupby(['job']).count()[['age']]*100/len(df.query('age_grp==2'))\n",
    "edu=pd.merge(left=hyp_4, right=educ_transform, left_on='job', right_on='genre_encoded_dumb')[['age','category','genre_encoded_dumb']]\n",
    "\n",
    "#hypothesis 5\n",
    "df.groupby(['overdraft','default'])['age'].count()\n",
    "#display(df[personal_info].corr().default.overdraft)\n",
    "\n",
    "#hypothesis 6\n",
    "df.groupby(['indebt','default'])['age'].count()\n",
    "\n",
    "##hypothesis 7\n",
    "hyp7=df.groupby(['job'])['age'].min()\n",
    "pd.merge(left=hyp7, right=educ_transform, left_on='job', right_on='genre_encoded_dumb')[['age','category','genre_encoded_dumb']]\n",
    "df.groupby(['married']).min().age\n",
    "\n",
    "##hypothesis 8 new\n",
    "df.groupby(['balance','default'])['age'].count()*100/df.groupby(['default'])['age'].count()\n",
    "\n",
    "##hypothesis 9\n",
    "hyp9=df.query('age<21').groupby(['job','housing'])['age'].count()*100/df.query('age<21').groupby(['job'])['age'].count()\n",
    "#display(hyp9)\n",
    "tmps=df.query('age<21').groupby(['job'])['age'].count()*100/df.query('age<21').housing.count()\n",
    "pd.merge(left=tmps, right=educ_transform, left_on='job', right_on='genre_encoded_dumb')[['category','genre_encoded_dumb']]\n",
    "\n",
    "hyp10=df.groupby(['job','education'])['age'].count()*100/df.groupby(['job'])['age'].count()\n",
    "#display(hyp10)\n",
    "#pd.merge(left=hyp7, right=educ_transform, left_on='job', right_on='genre_encoded_dumb')[['category','genre_encoded_dumb']]\n",
    "df.groupby(['age_grp'])['overdraft'].sum()#*100/df.query('age<21').groupby(['job'])['age'].count()\n",
    "\n",
    "### Personal standing financially: \n",
    "\n",
    "this_campaign_info=['campaign','contact','contact_missing','day','day_of_week','month','year', 'duration']\n",
    "\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "# #monday:1\n",
    "# for col in [\"day_of_week\"]:\n",
    "#     a=LabelEncoder().fit(df[col])\n",
    "#     print(df[col].unique(),list(a.classes_))\n",
    "#     df[col]=a.transform(df[col])\n",
    "\n",
    "mapping = {\n",
    "    'Sunday': 0,\n",
    "    'Monday': 1,\n",
    "    'Tuesday': 2,\n",
    "    'Wednesday': 3,\n",
    "    'Thursday': 4,\n",
    "    'Friday': 5,\n",
    "    'Saturday': 6,\n",
    "\n",
    "}\n",
    "\n",
    "df['day_of_week'] = df['day_of_week'].replace(mapping)\n",
    "df\n",
    "#df[this_campaign_info].info()\n",
    "\n",
    "\n",
    "corrmat = df[this_campaign_info].corr(method='pearson')\n",
    "display(corrmat)\n",
    "top_corr_features = corrmat.index\n",
    "plt.figure(figsize=(12,12))\n",
    "#plot heat map\n",
    "g=sns.heatmap(df[this_campaign_info][top_corr_features].corr(),annot=True,cmap=\"RdYlGn\")\n",
    "\n",
    "\n",
    "for c in this_campaign_info:\n",
    "    values=pd.DataFrame()\n",
    "    norm=pd.DataFrame()\n",
    "\n",
    "    values['yes']=df[df['y']=='yes'][c].value_counts().sort_values()\n",
    "    values['no']=df[df['y']=='no'][c].value_counts().sort_values()\n",
    "    #display(values)\n",
    "    norm['yes']=values['yes']/values.max(axis=1)\n",
    "    norm['no']=values['no']/values.max(axis=1)\n",
    "\n",
    "    #display(norm)\n",
    "\n",
    "\n",
    "    norm.plot(kind='bar')\n",
    "    plt.xlabel(f'{c}')\n",
    "    plt.ylabel('Number of clients')\n",
    "    plt.title('{}'.format(c))\n",
    "    plt.show();\n",
    "\n",
    "#sns.boxplot(df['duration'])\n",
    "df.duration.describe()\n",
    "for col in ['duration']:\n",
    "    #outliers['col']=col\n",
    "    upper_lim = df[col].quantile(.99)\n",
    "    print('{}''s statistics\\nupper limit={}\\nmax={}\\nlen(> upper_lim)={}\\n'.format(col,upper_lim,df[col].max(),len(df[df['duration']>upper_lim])))\n",
    "\n",
    "df['duration']=df['duration']/60\n",
    "sns.boxplot(df['duration'])\n",
    "\n",
    "corrmat = df[this_campaign_info].corr(method='pearson')\n",
    "display(corrmat)\n",
    "\n",
    "df['duration']\n",
    "bins_it=(df.duration.max()-df.duration.min())/5\n",
    "bins=[]#[df.balance.min()]\n",
    "for i in range(1,5):\n",
    "    bins.append(bins_it*i+1)\n",
    "\n",
    "bins.append(df.duration.max())\n",
    "#bins=[bins_middle,df.balance.max()]\n",
    "print('equally spaced bins would look like this',bins)\n",
    "\n",
    "df.duration.describe()\n",
    "\n",
    "rec_cond= (df['year']==2008 ) | ((df['year']==2009) & (df['month']<6))\n",
    "no_rec_cond= (df['year']==2010 ) | ((df['year']==2009) & (df['month']>=6))\n",
    "df.groupby(['year'])['age'].count()\n",
    "\n",
    "df.loc[rec_cond,'rec']=1\n",
    "df.loc[no_rec_cond,'rec']=0\n",
    "this_campaign_info.append('rec')\n",
    "\n",
    "df.rec=df.rec.astype('int32')\n",
    "#df.rec.head()\n",
    "a=df.groupby(['rec'])['duration'].agg(pd.Series.mode)\n",
    "b=df.groupby(['rec'])['campaign'].mean()\n",
    "ab=pd.merge(left=a, right=b, left_on=['rec'], right_on=['rec'])\n",
    "\n",
    "a=df.groupby(['year'])['duration'].agg(pd.Series.mode)\n",
    "b=df.groupby(['year'])['campaign'].mean()\n",
    "ab=pd.merge(left=a, right=b, left_on=['year'], right_on=['year'])\n",
    "ab\n",
    "\n",
    "cats=['day_of_week','month']\n",
    "ind=0\n",
    "for c in cats:\n",
    "    #c=cats[0]\n",
    "    #cunique=df[c].unique()\n",
    "    values=pd.DataFrame()\n",
    "    values['yes']=df[df['y']=='yes'][c].value_counts().sort_values()\n",
    "    values['no']=df[df['y']=='no'][c].value_counts().sort_values()\n",
    "    display(values)\n",
    "    values.plot(kind='bar')\n",
    "    plt.xlabel(f'{c}')\n",
    "    plt.ylabel('Number of clients')\n",
    "    plt.title('{}'.format(c))\n",
    "    plt.show();\n",
    "    \n",
    "# df.columns\n",
    "previous_contact=['pdays', 'previous','poutcome','poutcome_missing']\n",
    "\n",
    "\n",
    "corrmat = df[previous_contact].corr(method='pearson')\n",
    "#display(corrmat)\n",
    "top_corr_features = corrmat.index\n",
    "plt.figure()\n",
    "g=sns.heatmap(df[previous_contact][top_corr_features].corr(),annot=True,cmap=\"RdYlGn\")\n",
    "\n",
    "df.pdays.describe()\n",
    "df.previous.describe()\n",
    "\n",
    "for col in ['pdays','previous']:\n",
    "    #outliers['col']=col\n",
    "    upper_lim = df[col].quantile(.99)\n",
    "    print('{}''s statistics\\nvar={}\\nupper limit={}\\nmax={}\\nlen(> upper_lim)={}\\n'.format(col,df[col].var(),upper_lim,df[col].max(),len(df[df['duration']>upper_lim])))\n",
    "    \n",
    "\n",
    "print('# of entries where pdays>0={}\\n# of entries where pdays=0={}'.format(len(df.query(\"pdays>0\")), len(df.query(\"pdays==0\"))))#.groupby('y_equiv').count()\n",
    "print('\\n# of entries where previous>0={}\\n# of entries where previous=0={}'.format(len(df.query(\"previous>0\")), len(df.query(\"previous==0\"))))#.groupby('y_equiv').count()\n",
    "\n",
    "\n",
    "forppt=['previous','poutcome']\n",
    "corrmat = df[forppt].corr(method='pearson')\n",
    "display(corrmat)\n",
    "top_corr_features = corrmat.index\n",
    "plt.figure()\n",
    "g=sns.heatmap(df[forppt][top_corr_features].corr(),annot=True,cmap=\"RdYlGn\")\n",
    "\n",
    "for c in ['poutcome']:\n",
    "    values=pd.DataFrame()\n",
    "    norm=pd.DataFrame()\n",
    "\n",
    "    values['yes']=df[df['y']=='yes'][c].value_counts().sort_values()\n",
    "    values['no']=df[df['y']=='no'][c].value_counts().sort_values()\n",
    "    display(values)\n",
    "    norm['yes']=values['yes']/values.max(axis=1)\n",
    "    norm['no']=values['no']/values.max(axis=1)\n",
    "    \n",
    "    norm.plot(kind='bar')\n",
    "    plt.xlabel(f'{c}')\n",
    "    plt.ylabel('Number of clients')\n",
    "    plt.title('{}'.format(c))\n",
    "    plt.show();\n",
    "    \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "df.euribor3m.sort_values()\n",
    "\n",
    "df.groupby(['y','euribor3m']).count()\n",
    "\n",
    "#plt.plot(df[df['y']=='yes'].euribor3m.sort_values())#.value_counts()\n",
    "#df.groupby(df[df['y']=='yes'].euribor3m).count().age.sort_values()\n",
    "\n",
    "filtered=df[df['y']=='yes'].euribor3m.sort_values()\n",
    "#plt.plot(df.groupby(filtered).count().age)\n",
    "\n",
    "#plt.plot(df.groupby(filtered).count().age)\n",
    "x=df.groupby(filtered).count()[['age']].index.tolist() #euribor3m\n",
    "y=df.groupby(filtered).count()['age'].values.tolist() #counts \n",
    "# plt.plot(x,y)\n",
    "# plt.show()\n",
    "plt.plot([yi/df.groupby(filtered).count()['age'].max() for yi in y])\n",
    "\n",
    "\n",
    "filtered=df[df['y']=='no'].euribor3m.sort_values()#/df[df['y']=='no'].euribor3m.sort_values()\n",
    "x=df.groupby(filtered).count()[['age']].index.tolist() #euribor3m\n",
    "y=df.groupby(filtered).count()['age'].values.tolist() #counts \n",
    "plt.plot([yi/df.groupby(filtered).count()['age'].max() for yi in y])\n",
    "\n",
    "##################################################\n",
    "\n",
    "df.columns\n",
    "eco_indic=['euribor3m','euribor3m_ffill','consum_prices_rate', 'consum_conf_ind', 'employed', 'unemployed','unemployed_rate']\n",
    "\n",
    "corrmat = df[eco_indic].corr(method='pearson')\n",
    "display(corrmat)\n",
    "top_corr_features = corrmat.index\n",
    "plt.figure()\n",
    "g=sns.heatmap(df[eco_indic][top_corr_features].corr(),annot=True,cmap=\"RdYlGn\")\n",
    "df[eco_indic].describe()\n",
    "\n",
    "dff['age_grp']=df['age_grp'] \n",
    "dff['rec']=df['rec']\n",
    "dff['indebt']=df['indebt']\n",
    "dff['balance']=df['balance']\n",
    "#dff['pdays']=df['pdays']\n",
    "#dff['previous']=df['previous']\n",
    "dff['duration']=df['duration']\n",
    "\n",
    "dff.describe()\n",
    "\n",
    "dff.columns\n",
    "\n",
    "dff.to_csv(\"week10_pptEDA.csv\",\n",
    "          index=False,\n",
    "          encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "df_train=pd.read_csv('train_set.csv') #to get train indices \n",
    "df_test=pd.read_csv('test_set.csv') #to get test indices \n",
    "\n",
    "df=pd.read_csv('week10_pptEDA.csv')\n",
    "\n",
    "toremove=['day','month','year']#'pdays','employed','unemployed','year']#,'previous']\n",
    "df.drop(toremove,axis=1,inplace=True)\n",
    "\n",
    "train=df.loc[df_train.idx.tolist()]\n",
    "test=df.loc[df_test.idx.tolist()]\n",
    "display(df.columns)\n",
    "\n",
    "non_categoric=['age', 'euribor3m', 'consum_prices_rate', 'consum_conf_ind', 'unemployed_rate','employed','unemployed','pdays','previous','campaign','duration']\n",
    "targets=['y','y_equiv']\n",
    "allbutcat=targets+non_categoric\n",
    "categoric=df.columns[~df.columns.isin(allbutcat)].tolist()\n",
    "features=df.columns[~df.columns.isin(targets)].tolist()\n",
    "\n",
    "fig, axs = plt.subplots(nrows=len(non_categoric), ncols=4, figsize=(15, 30))\n",
    "fig.subplots_adjust(wspace=0.9, hspace=0.9)\n",
    "\n",
    "df_log=pd.DataFrame()\n",
    "\n",
    "tocheckoutliers=['age','pdays','previous','campaign','duration']\n",
    "for n in non_categoric:\n",
    "    #print(n)\n",
    "    idx=non_categoric.index(n)\n",
    "    \n",
    "    IQR=df[n].quantile(.75)-df[n].quantile(.25)\n",
    "    lower=df[n].quantile(.25) - (1.5*IQR)# (Q1 - 1.5 * IQR)\n",
    "    upper=df[n].quantile(.75) + (1.5*IQR)# (Q3 + 1.5 * IQR\n",
    "    \n",
    "    info=str('{}\\nmin={:.2f},mean={:.2f},std={:.2f}\\nkurtosis={:.2f},outliers={:.2f},max={:.2f}\\nskew={:.2f}'.format(n,df[n].min(),df[n].mean(),df[n].std(),df[n].kurtosis(),upper,df[n].max(),df[n].skew()))    \n",
    "    \n",
    "    df[n].hist(ax=axs[idx,0])\n",
    "    axs[idx,0].set_title(info)\n",
    "    \n",
    "    \n",
    "    \n",
    "    df_log[n]=np.log2(df[n]-(df[n].min()-1))\n",
    "    \n",
    "    IQR=df_log[n].quantile(.75)-df_log[n].quantile(.25)\n",
    "    lower=df_log[n].quantile(.25) - (1.5*IQR)# (Q1 - 1.5 * IQR)\n",
    "    upper=df_log[n].quantile(.75) + (1.5*IQR)# (Q3 + 1.5 * IQR)\n",
    "    \n",
    "    #print(n,df_log[n].min(),lower,upper,df_log[n].max())\n",
    "    \n",
    "    info_log=str('{}\\nmin={:.2f},mean={:.2f},std={:.2f}\\nkurtosis={:.2f},outliers={:.2f},max={:.2f}\\nskew={:.2f}'.format(n,df_log[n].min(),df_log[n].mean(),df_log[n].std(),df_log[n].kurtosis(),upper,df_log[n].max(),df_log[n].skew()))    \n",
    "    \n",
    "    \n",
    "    df_log[n].hist(ax=axs[idx,1])\n",
    "    axs[idx,1].set_title(info_log)\n",
    "\n",
    "    df.boxplot(column=n,ax=axs[idx,2])\n",
    "\n",
    "\n",
    "print('Do not touch categoric columns nor as transformation nor scaling\\n Do not remove or scale what seems like outliers from economic indicators ')\n",
    "\n",
    "train_log=df_log.loc[df_train.idx.tolist()]\n",
    "test_log=df_log.loc[df_test.idx.tolist()]\n",
    "\n",
    "\n",
    "outliers_tohandle=['campaign','duration']\n",
    "\n",
    "df_o=pd.DataFrame()\n",
    "\n",
    "for n in outliers_tohandle:\n",
    "    idx=non_categoric.index(n)\n",
    "    robust=RobustScaler(quantile_range=(1,99)).fit(train_log[[n]])#\n",
    "    df_o[n]=robust.transform(df_log[[n]]).flatten()\n",
    "\n",
    "    IQR=df_o[n].quantile(.75)-df_log[n].quantile(.25)\n",
    "    lower=df_o[n].quantile(.25) - (1.5*IQR)# (Q1 - 1.5 * IQR)\n",
    "    upper=df_o[n].quantile(.75) + (1.5*IQR)# (Q3 + 1.5 * IQR)\n",
    "    \n",
    "    info_o=str('{}\\nmin={:.2f},mean={:.2f},std={:.2f}\\nkurtosis={:.2f},outliers={:.2f},max={:.2f}\\nskew={:.2f}'.format(n,df_o[n].min(),df_o[n].mean(),df_o[n].std(),df_o[n].kurtosis(),upper,df_o[n].max(),df_o[n].skew()))    \n",
    "    axs[idx,3].set_title(info_o)\n",
    "\n",
    "    df_o[n].hist(ax=axs[idx,3])\n",
    "\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "df_log[outliers_tohandle]=df_o[outliers_tohandle]\n",
    "\n",
    "\n",
    "df_opt=df[df.columns[~df.columns.isin(non_categoric)]].join(df_log)\n",
    "\n",
    "# toignore=problematic+non_categoric\n",
    "# print(toignore)#=problematic+non_categoric\n",
    "\n",
    "# df_logs[problematic]=df[problematic]\n",
    "\n",
    "# df_opt=df[df.columns[~df.columns.isin(toignore)]].join(df_logs)\n",
    "\n",
    "\n",
    "df_opt.drop('pdays',axis=1,inplace=True)\n",
    "non_categoric.remove('pdays')\n",
    "\n",
    "print('transformed and scaled statistics of features/n',df_opt[non_categoric].describe())\n",
    "\n",
    "# ################tc whether to do this step now or after log ????????????????????????????\n",
    "# train=df_opt.loc[df_train.idx.tolist()]\n",
    "# test=df_opt.loc[df_test.idx.tolist()]\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.utils.validation import check_is_fitted, check_consistent_length\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.utils import check_matplotlib_support\n",
    "from sklearn.base import is_classifier\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    plot_confusion_matrix,\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    log_loss,\n",
    "    roc_auc_score,\n",
    "    SCORERS\n",
    ")\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression #baseline for binary classification problems\n",
    "from sklearn.ensemble import RandomForestClassifier # weak learners \n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "import time\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "\n",
    "train=df_opt.loc[df_train.idx.tolist()]\n",
    "test=df_opt.loc[df_test.idx.tolist()]\n",
    "\n",
    "X_train=train[train.columns[~train.columns.isin(['y', 'y_equiv','duration'])]].values\n",
    "y_train=train.loc[:,(train.columns =='y_equiv')].y_equiv.astype('int32')\n",
    "\n",
    "X_test=test[test.columns[~test.columns.isin(['y', 'y_equiv','duration'])]].values\n",
    "y_test=test.loc[:,(test.columns =='y_equiv')].y_equiv.astype('int32')\n",
    "\n",
    "scores = defaultdict(list)\n",
    "\n",
    "\n",
    "## logistic regression --> scale non-categoric with minmax since non is gaussian except duration which isn't contextually correct to use anyway\n",
    "#scaling necessary because of penality L2 that normalizes across rows not columns so they need to be relatable \n",
    "non_categoric.remove('duration')\n",
    "df_lr=df_opt\n",
    "# for i in non_categoric:\n",
    "#     minmax=MinMaxScaler()\n",
    "#     minmaxfit=minmax.fit(train[[i]])\n",
    "#     tmp=minmax.transform(df_opt[i].values.reshape(-1, 1))\n",
    "# #    info=str('{}\\nmin={:.2f},mean={:.2f},std={:.2f}\\nkurtosis={:.2f},outliers={:.2f},max={:.2f}\\nskew={:.2f}'.format(i,df_opt[i].min(),df_opt[i].mean(),df_opt[i].std(),df_opt[i].kurtosis(),df_opt[i].quantile(.99),df_opt[i].max(),df_opt[i].skew()))      \n",
    "# #     print(info,'\\n')\n",
    "#     df_lr[i]=tmp\n",
    "features=df_lr.columns[~df_lr.columns.isin(['y', 'y_equiv'])]\n",
    "for i in features.tolist():\n",
    "    minmax=MinMaxScaler()\n",
    "    minmaxfit=minmax.fit(train[[i]].values)\n",
    "    tmp=minmax.transform(df_opt[i].values.reshape(-1, 1))\n",
    "#    info=str('{}\\nmin={:.2f},mean={:.2f},std={:.2f}\\nkurtosis={:.2f},outliers={:.2f},max={:.2f}\\nskew={:.2f}'.format(i,df_opt[i].min(),df_opt[i].mean(),df_opt[i].std(),df_opt[i].kurtosis(),df_opt[i].quantile(.99),df_opt[i].max(),df_opt[i].skew()))      \n",
    "#     print(info,'\\n')\n",
    "    df_lr[i]=tmp\n",
    "\n",
    "    \n",
    "train_lr=df_lr.loc[df_train.idx.tolist()]\n",
    "test_lr=df_lr.loc[df_test.idx.tolist()]\n",
    "\n",
    "X_train_lr=train_lr[train_lr.columns[~train_lr.columns.isin(['y', 'y_equiv','duration'])]].values\n",
    "y_train_lr=train_lr.loc[:,(train_lr.columns =='y_equiv')].y_equiv.astype('int32')\n",
    "\n",
    "X_test_lr=test_lr[test_lr.columns[~test_lr.columns.isin(['y', 'y_equiv','duration'])]].values\n",
    "y_test_lr=test_lr.loc[:,(test_lr.columns =='y_equiv')].y_equiv.astype('int32')\n",
    "###############################################################\n",
    "\n",
    "n_jobs = -1\n",
    "random_state=11\n",
    "\n",
    "clf=LogisticRegression(random_state=random_state, n_jobs=n_jobs, max_iter=4000)\n",
    "pipe=Pipeline(steps=[('lr',clf)])\n",
    "\n",
    "cv= RepeatedStratifiedKFold(n_splits=6, n_repeats=3, random_state=random_state)\n",
    "#cv = StratifiedKFold(shuffle=True, n_splits=6, random_state=random_state)\n",
    "\n",
    "\n",
    "# LogisticRegression().get_params().keys()\n",
    "param_grid = [{\n",
    "                'lr__penalty': ['l2','none'],\n",
    "                'lr__class_weight': ['balanced'],\n",
    "                'lr__solver': ['sag','newton-cg'],\n",
    "                'lr__multi_class': ['multinomial','ovr'],\n",
    "}]\n",
    "\n",
    "\n",
    "gs=GridSearchCV(pipe, param_grid, scoring='precision',cv=cv)\n",
    "gs.fit(X_train_lr, y_train_lr)\n",
    "\n",
    "print('best parameters are',gs.best_params_)\n",
    "start = time.time()\n",
    "\n",
    "y_predict=gs.predict(X_test_lr)\n",
    "y_prob = gs.predict_proba(X_test_lr) #model.predict_proba(X_test)[:,1]\n",
    "\n",
    "\n",
    "scores[\"Classifier\"].append('lr')\n",
    "\n",
    "for metric in [log_loss]:\n",
    "    score_name = metric.__name__.replace(\"_\", \" \").replace(\"score\", \"\").capitalize()\n",
    "    scores[score_name].append(metric(y_test_lr, y_prob[:, 1]))\n",
    "\n",
    "\n",
    "for metric in [accuracy_score,precision_score, recall_score, f1_score, roc_auc_score,r2_score]:#,calibration_curve_ece]:\n",
    "    score_name = metric.__name__.replace(\"_\", \" \").replace(\"score\", \"\").capitalize()\n",
    "    scores[score_name].append(metric(y_test_lr, y_predict))\n",
    "\n",
    "scores\n",
    "end = time.time()\n",
    "scores['time'].append(end-start)\n",
    "scores['bestmodel'].append(gs.best_params_)\n",
    "\n",
    "\n",
    "score_df = pd.DataFrame(scores).set_index(\"Classifier\")\n",
    "score_df.round(decimals=3)\n",
    "\n",
    "score_df  \n",
    "#y_prob[:,1]=y_predict\n",
    "\n",
    "## logisitc regression train w/o missing entries , expected to improve \n",
    "\n",
    "cols=['contact','poutcome','job','education','euribor3m']\n",
    "\n",
    "X_miss=train#_lr\n",
    "locs=[]\n",
    "\n",
    "for c in cols:\n",
    "    if c=='euribor3m':\n",
    "        cmiss=c+'_ffill'\n",
    "    else:\n",
    "        cmiss=c+'_missing'\n",
    "    tmp=X_miss.loc[(X_miss[c]==0) & (X_miss[cmiss]==1)].index.tolist()\n",
    "    \n",
    "\n",
    "    locs.append(tmp)\n",
    "\n",
    "toskiptrain=set(sorted(sum(locs, [])))\n",
    "\n",
    "print('{} samples from {} training samples have no-missing data'.format(-1*(len(toskiptrain)-len(train_lr)),len(train_lr)))\n",
    "\n",
    "\n",
    "train_clipped=train_lr.drop(toskiptrain,axis=0)\n",
    "train_clipped\n",
    "\n",
    "X_train_clipped=train_clipped[train_clipped.columns[~train_clipped.columns.isin(['y', 'y_equiv','duration'])]].values\n",
    "y_train_clipped=train_clipped.loc[:,(train_clipped.columns =='y_equiv')].y_equiv.astype('int32')\n",
    "\n",
    "# clf2=gs.best_estimator_['lr']\n",
    "# # pipe=Pipeline(StandardScaler(),clf2)\n",
    "\n",
    "# pipe=make_pipeline(clf2)\n",
    "# pipe.fit(X_train_clipped, y_train_clipped)  # apply scaling on training data\n",
    "########################################################################################3\n",
    "\n",
    "clf=LogisticRegression(random_state=random_state, n_jobs=n_jobs, max_iter=4000)\n",
    "pipe=Pipeline(steps=[('lr',clf)])\n",
    "\n",
    "cv= RepeatedStratifiedKFold(n_splits=6, n_repeats=3, random_state=random_state)\n",
    "#cv = StratifiedKFold(shuffle=True, n_splits=6, random_state=random_state)\n",
    "\n",
    "# LogisticRegression().get_params().keys()\n",
    "param_grid = [{\n",
    "                'lr__penalty': ['l2','none'],\n",
    "                'lr__class_weight': ['balanced'],\n",
    "                'lr__solver': ['sag','newton-cg'],\n",
    "                'lr__multi_class': ['multinomial','ovr'],\n",
    "}]\n",
    "\n",
    "\n",
    "gs=GridSearchCV(pipe, param_grid, scoring='precision',cv=cv)\n",
    "gs.fit(X_train_clipped, y_train_clipped)\n",
    "\n",
    "##########################################################################################\n",
    "\n",
    "\n",
    "\n",
    "print('best parameters are',gs.best_params_)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "y_predict=gs.predict(X_test_lr)\n",
    "\n",
    "y_prob = gs.predict_proba(X_test_lr)\n",
    "\n",
    "scores[\"Classifier\"].append('lr-missingvalues')\n",
    "\n",
    "for metric in [log_loss]:\n",
    "    score_name = metric.__name__.replace(\"_\", \" \").replace(\"score\", \"\").capitalize()\n",
    "    scores[score_name].append(metric(y_test_lr, y_prob[:, 1]))\n",
    "\n",
    "\n",
    "for metric in [accuracy_score,precision_score, recall_score, f1_score, roc_auc_score,r2_score]:#,calibration_curve_ece]:\n",
    "    score_name = metric.__name__.replace(\"_\", \" \").replace(\"score\", \"\").capitalize()\n",
    "    scores[score_name].append(metric(y_test_lr, y_predict))\n",
    "\n",
    "end = time.time()\n",
    "scores['time'].append(end-start)\n",
    "scores['bestmodel'].append(gs.best_params_)\n",
    "\n",
    "score_df = pd.DataFrame(scores).set_index(\"Classifier\")\n",
    "score_df.round(decimals=3)\n",
    "\n",
    "score_df  \n",
    "\n",
    "##random forest \n",
    "\n",
    "n_jobs = -1\n",
    "random_state=11\n",
    "\n",
    "clf=RandomForestClassifier(n_jobs=n_jobs,random_state=random_state)#20 is nothing, std is 100,min_samples_split 5? default=2,min_samples_leafint 3? default=1 \n",
    "\n",
    "pipe=Pipeline(steps=[('rf',clf)])\n",
    "\n",
    "cv= RepeatedStratifiedKFold(n_splits=6, n_repeats=3, random_state=random_state)\n",
    "#cv = StratifiedKFold(shuffle=True, n_splits=6, random_state=random_state)\n",
    "\n",
    "## ince you are using scikit, try gradient boosted trees on your data. You'll probably get better precision-recall AUC right out of the box. SVCs, as you point out, are not really practical for anything but very small datasets.\n",
    "\n",
    "# LogisticRegression().get_params().keys()\n",
    "param_grid = [{\n",
    "                'rf__n_estimators': [150],\n",
    "                'rf__max_depth':[len(X_train/2),3*len(X_train)/4],\n",
    "                'rf__criterion': ['gini','entropy'],\n",
    "                'rf__class_weight': ['balanced', 'balanced_subsample'],\n",
    "                'rf__oob_score':[True],  \n",
    "                'rf__max_samples':[0.2,0.4],\n",
    "}]\n",
    "\n",
    "\n",
    "gs=GridSearchCV(pipe, param_grid, scoring='precision',cv=cv)\n",
    "\n",
    "gs.fit(X_train, y_train)\n",
    "\n",
    "print('best parameters are',gs.best_params_)\n",
    "start = time.time()\n",
    "\n",
    "y_predict=gs.predict(X_test)\n",
    "y_prob = gs.predict_proba(X_test) #model.predict_proba(X_test)[:,1]\n",
    "\n",
    "#scores = defaultdict(list)\n",
    "\n",
    "scores[\"Classifier\"].append('rf')\n",
    "\n",
    "for metric in [log_loss]:\n",
    "    score_name = metric.__name__.replace(\"_\", \" \").replace(\"score\", \"\").capitalize()\n",
    "    scores[score_name].append(metric(y_test, y_prob[:, 1]))\n",
    "\n",
    "\n",
    "for metric in [accuracy_score,precision_score, recall_score, f1_score, roc_auc_score,r2_score]:#,calibration_curve_ece]:\n",
    "    score_name = metric.__name__.replace(\"_\", \" \").replace(\"score\", \"\").capitalize()\n",
    "    scores[score_name].append(metric(y_test, y_predict))\n",
    "\n",
    "end = time.time()\n",
    "scores['time'].append(end-start)\n",
    "scores['bestmodel'].append(gs.best_params_)\n",
    "\n",
    "score_df = pd.DataFrame(scores).set_index(\"Classifier\")\n",
    "score_df.round(decimals=3)\n",
    "gs_rf=gs\n",
    "score_df  \n",
    "\n",
    "## bagging classifier\n",
    "\n",
    "n_jobs = -1\n",
    "random_state=11\n",
    "from imblearn.ensemble import BalancedBaggingClassifier \n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "                               \n",
    "clf3=gs.best_estimator_['rf']\n",
    "# clf3=score_df.loc['rf'].bestmodel\n",
    "bag=BalancedBaggingClassifier(base_estimator=clf3,replacement=False,\n",
    "                                random_state=random_state,\n",
    "                             n_jobs=n_jobs)\n",
    "\n",
    "pipe=Pipeline(steps=[('bag',bag)])\n",
    "\n",
    "cv= RepeatedStratifiedKFold(n_splits=6, n_repeats=3, random_state=random_state)\n",
    "# cv = StratifiedKFold(shuffle=True, n_splits=6, random_state=random_state)\n",
    "\n",
    "## ince you are using scikit, try gradient boosted trees on your data. You'll probably get better precision-recall AUC\n",
    "#right out of the box. SVCs, as you point out, are not really practical for anything but very small datasets.\n",
    "\n",
    "# LogisticRegression().get_params().keys()\n",
    "param_grid = [{\n",
    "                'bag__sampling_strategy':['all','not majority','majority'],\n",
    "}]\n",
    "\n",
    "gs=GridSearchCV(pipe, param_grid, scoring='precision',cv=cv)\n",
    "\n",
    "gs.fit(X_train, y_train)\n",
    "\n",
    "print('best parameters are',gs.best_params_)\n",
    "start = time.time()\n",
    "y_predict=gs.predict(X_test)\n",
    "y_prob = gs.predict_proba(X_test) #model.predict_proba(X_test)[:,1]\n",
    "\n",
    "#scores = defaultdict(list)\n",
    "\n",
    "scores[\"Classifier\"].append('bag_rf')\n",
    "\n",
    "for metric in [log_loss]:\n",
    "    score_name = metric.__name__.replace(\"_\", \" \").replace(\"score\", \"\").capitalize()\n",
    "    scores[score_name].append(metric(y_test, y_prob[:, 1]))\n",
    "\n",
    "\n",
    "for metric in [accuracy_score,precision_score, recall_score, f1_score, roc_auc_score,r2_score]:#,calibration_curve_ece]:\n",
    "    score_name = metric.__name__.replace(\"_\", \" \").replace(\"score\", \"\").capitalize()\n",
    "    scores[score_name].append(metric(y_test, y_predict))\n",
    "\n",
    "end = time.time()\n",
    "scores['time'].append(end-start)\n",
    "scores['bestmodel'].append(gs.best_params_)\n",
    "\n",
    "score_df = pd.DataFrame(scores).set_index(\"Classifier\")\n",
    "score_df.round(decimals=3)\n",
    "\n",
    "score_df     \n",
    "\n",
    "n_jobs = -1\n",
    "random_state=11\n",
    "\n",
    "clf=RandomForestClassifier(n_jobs=n_jobs,random_state=random_state)#20 is nothing, std is 100,min_samples_split 5? default=2,min_samples_leafint 3? default=1 \n",
    "\n",
    "pipe=Pipeline(steps=[('rf',clf)])\n",
    "\n",
    "cv= RepeatedStratifiedKFold(n_splits=6, n_repeats=3, random_state=random_state)\n",
    "# cv = StratifiedKFold(shuffle=True, n_splits=6, random_state=random_state)\n",
    "\n",
    "## ince you are using scikit, try gradient boosted trees on your data. You'll probably get better precision-recall AUC right out of the box. SVCs, as you point out, are not really practical for anything but very small datasets.\n",
    "train_clipped=train.drop(toskiptrain,axis=0)\n",
    "train_clipped\n",
    "\n",
    "X_train_clipped=train_clipped[train_clipped.columns[~train_clipped.columns.isin(['y', 'y_equiv','duration'])]].values\n",
    "y_train_clipped=train_clipped.loc[:,(train_clipped.columns =='y_equiv')].y_equiv.astype('int32')\n",
    "\n",
    "# LogisticRegression().get_params().keys()\n",
    "param_grid = [{\n",
    "                'rf__n_estimators': [150],\n",
    "                'rf__max_depth':[len(X_train_clipped)/2,3*len(X_train_clipped)/4],\n",
    "                'rf__criterion': ['gini','entropy'],\n",
    "                'rf__class_weight': ['balanced', 'balanced_subsample'],\n",
    "                'rf__oob_score':[True],  \n",
    "                'rf__max_samples':[0.2,0.4],\n",
    "}]\n",
    "\n",
    "\n",
    "gs=GridSearchCV(pipe, param_grid, scoring='precision',cv=cv)\n",
    "\n",
    "gs.fit(X_train_clipped, y_train_clipped)\n",
    "\n",
    "print('best parameters are',gs.best_params_)\n",
    "start = time.time()\n",
    "\n",
    "y_predict=gs.predict(X_test)\n",
    "y_prob = gs.predict_proba(X_test) #model.predict_proba(X_test)[:,1]\n",
    "\n",
    "#scores = defaultdict(list)\n",
    "\n",
    "scores[\"Classifier\"].append('rf_missingvalues')\n",
    "\n",
    "for metric in [log_loss]:\n",
    "    score_name = metric.__name__.replace(\"_\", \" \").replace(\"score\", \"\").capitalize()\n",
    "    scores[score_name].append(metric(y_test, y_prob[:, 1]))\n",
    "\n",
    "\n",
    "for metric in [accuracy_score,precision_score, recall_score, f1_score, roc_auc_score,r2_score]:#,calibration_curve_ece]:\n",
    "    score_name = metric.__name__.replace(\"_\", \" \").replace(\"score\", \"\").capitalize()\n",
    "    scores[score_name].append(metric(y_test, y_predict))\n",
    "\n",
    "end = time.time()\n",
    "scores['time'].append(end-start)\n",
    "scores['bestmodel'].append(gs.best_params_)\n",
    "\n",
    "score_df = pd.DataFrame(scores).set_index(\"Classifier\")\n",
    "score_df.round(decimals=3)\n",
    "\n",
    "score_df\n",
    "\n",
    "from collections import Counter\n",
    "counter = Counter(y_train)\n",
    "display(counter)\n",
    "# estimate scale_pos_weight value\n",
    "#estimate = counter[0] / counter[1]\n",
    "#print('Estimate: %.3f' % estimate)\n",
    "\n",
    "for metric in [accuracy_score,precision_score, recall_score, f1_score, roc_auc_score,r2_score]:#,calibration_curve_ece]:\n",
    "    score_name = metric.__name__.replace(\"_\", \" \").replace(\"score\", \"\").capitalize()\n",
    "    print(score_name)\n",
    "    #scores[score_name].append(metric(y_test, y_predict))\n",
    "    \n",
    "##xgb boost \n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "counter = Counter(y_train)\n",
    "display(counter)\n",
    "# estimate scale_pos_weight value\n",
    "estimate = counter[0] / counter[1]\n",
    "print('Estimate: %.3f' % estimate)\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "# weights = [1, 10, 25, 50, 75, 99, 100, 1000]\n",
    "# param_grid = dict(scale_pos_weight=weights)\n",
    "\n",
    "n_jobs = -1\n",
    "random_state=11\n",
    "\n",
    "clf=XGBClassifier(n_jobs=n_jobs,random_state=random_state)#20 is nothing, std is 100,min_samples_split 5? default=2,min_samples_leafint 3? default=1 \n",
    "\n",
    "pipe=Pipeline(steps=[ ('xgb',clf)])\n",
    "\n",
    "cv= RepeatedStratifiedKFold(n_splits=6, n_repeats=3, random_state=random_state)\n",
    "# cv = StratifiedKFold(shuffle=True, n_splits=6, random_state=random_state)\n",
    "\n",
    "## ince you are using scikit, try gradient boosted trees on your data. You'll probably get better precision-recall AUC right out of the box. SVCs, as you point out, are not really practical for anything but very small datasets.\n",
    "\n",
    "# LogisticRegression().get_params().keys()\n",
    "param_grid = [{\n",
    "    'xgb__scale_pos_weight': [2/estimate,3/estimate,5/estimate,estimate**(1./5),estimate**(1./3),estimate**(1./2),estimate]\n",
    "}]\n",
    "\n",
    "\n",
    "gs=GridSearchCV(pipe, param_grid, scoring='precision',cv=cv)\n",
    "\n",
    "gs.fit(X_train, y_train)\n",
    "\n",
    "print('best parameters are',gs.best_params_)\n",
    "start = time.time()\n",
    "\n",
    "y_predict=gs.predict(X_test)\n",
    "y_prob = gs.predict_proba(X_test) #model.predict_proba(X_test)[:,1]\n",
    "\n",
    "#scores = defaultdict(list)\n",
    "\n",
    "scores[\"Classifier\"].append('xgb')\n",
    "\n",
    "for metric in [log_loss]:\n",
    "    score_name = metric.__name__.replace(\"_\", \" \").replace(\"score\", \"\").capitalize()\n",
    "    scores[score_name].append(metric(y_test, y_prob[:, 1]))\n",
    "\n",
    "\n",
    "for metric in [accuracy_score,precision_score, recall_score, f1_score, roc_auc_score,r2_score]:#,calibration_curve_ece]:\n",
    "    score_name = metric.__name__.replace(\"_\", \" \").replace(\"score\", \"\").capitalize()\n",
    "    scores[score_name].append(metric(y_test, y_predict))\n",
    "\n",
    "end = time.time()\n",
    "scores['time'].append(end-start)\n",
    "scores['bestmodel'].append(gs.best_params_)\n",
    "\n",
    "score_df = pd.DataFrame(scores).set_index(\"Classifier\")\n",
    "score_df.round(decimals=3)\n",
    "\n",
    "score_df  \n",
    "\n",
    "score_df.to_csv(\"final_models_results.csv\",\n",
    "          index=False,\n",
    "          encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "df_train=pd.read_csv('train_set.csv') #to get train indices \n",
    "df_test=pd.read_csv('test_set.csv') #to get test indices \n",
    "\n",
    "df=pd.read_csv('week10_pptEDA.csv')\n",
    "\n",
    "toremove=['day','month','year']#'pdays','employed','unemployed','year']#,'previous']\n",
    "df.drop(toremove,axis=1,inplace=True)\n",
    "\n",
    "train=df.loc[df_train.idx.tolist()]\n",
    "test=df.loc[df_test.idx.tolist()]\n",
    "display(df.columns)\n",
    "\n",
    "non_categoric=['age', 'euribor3m', 'consum_prices_rate', 'consum_conf_ind', 'unemployed_rate','employed','unemployed','pdays','previous','campaign','duration']\n",
    "targets=['y','y_equiv']\n",
    "allbutcat=targets+non_categoric\n",
    "categoric=df.columns[~df.columns.isin(allbutcat)].tolist()\n",
    "features=df.columns[~df.columns.isin(targets)].tolist()\n",
    "\n",
    "fig, axs = plt.subplots(nrows=len(non_categoric), ncols=4, figsize=(15, 30))\n",
    "fig.subplots_adjust(wspace=0.9, hspace=0.9)\n",
    "\n",
    "df_log=pd.DataFrame()\n",
    "\n",
    "tocheckoutliers=['age','pdays','previous','campaign','duration']\n",
    "for n in non_categoric:\n",
    "    #print(n)\n",
    "    idx=non_categoric.index(n)\n",
    "    \n",
    "    IQR=df[n].quantile(.75)-df[n].quantile(.25)\n",
    "    lower=df[n].quantile(.25) - (1.5*IQR)# (Q1 - 1.5 * IQR)\n",
    "    upper=df[n].quantile(.75) + (1.5*IQR)# (Q3 + 1.5 * IQR\n",
    "    \n",
    "    info=str('{}\\nmin={:.2f},mean={:.2f},std={:.2f}\\nkurtosis={:.2f},outliers={:.2f},max={:.2f}\\nskew={:.2f}'.format(n,df[n].min(),df[n].mean(),df[n].std(),df[n].kurtosis(),upper,df[n].max(),df[n].skew()))    \n",
    "    \n",
    "    df[n].hist(ax=axs[idx,0])\n",
    "    axs[idx,0].set_title(info)\n",
    "    \n",
    "    \n",
    "    \n",
    "    df_log[n]=np.log2(df[n]-(df[n].min()-1))\n",
    "    \n",
    "    IQR=df_log[n].quantile(.75)-df_log[n].quantile(.25)\n",
    "    lower=df_log[n].quantile(.25) - (1.5*IQR)# (Q1 - 1.5 * IQR)\n",
    "    upper=df_log[n].quantile(.75) + (1.5*IQR)# (Q3 + 1.5 * IQR)\n",
    "    \n",
    "    #print(n,df_log[n].min(),lower,upper,df_log[n].max())\n",
    "    \n",
    "    info_log=str('{}\\nmin={:.2f},mean={:.2f},std={:.2f}\\nkurtosis={:.2f},outliers={:.2f},max={:.2f}\\nskew={:.2f}'.format(n,df_log[n].min(),df_log[n].mean(),df_log[n].std(),df_log[n].kurtosis(),upper,df_log[n].max(),df_log[n].skew()))    \n",
    "    \n",
    "    \n",
    "    df_log[n].hist(ax=axs[idx,1])\n",
    "    axs[idx,1].set_title(info_log)\n",
    "\n",
    "    df.boxplot(column=n,ax=axs[idx,2])\n",
    "\n",
    "\n",
    "print('Do not touch categoric columns nor as transformation nor scaling\\n Do not remove or scale what seems like outliers from economic indicators ')\n",
    "\n",
    "train_log=df_log.loc[df_train.idx.tolist()]\n",
    "test_log=df_log.loc[df_test.idx.tolist()]\n",
    "\n",
    "\n",
    "outliers_tohandle=['campaign','duration']\n",
    "\n",
    "df_o=pd.DataFrame()\n",
    "\n",
    "for n in outliers_tohandle:\n",
    "    idx=non_categoric.index(n)\n",
    "    robust=RobustScaler(quantile_range=(1,99)).fit(train_log[[n]])#\n",
    "    df_o[n]=robust.transform(df_log[[n]]).flatten()\n",
    "\n",
    "    IQR=df_o[n].quantile(.75)-df_log[n].quantile(.25)\n",
    "    lower=df_o[n].quantile(.25) - (1.5*IQR)# (Q1 - 1.5 * IQR)\n",
    "    upper=df_o[n].quantile(.75) + (1.5*IQR)# (Q3 + 1.5 * IQR)\n",
    "    \n",
    "    info_o=str('{}\\nmin={:.2f},mean={:.2f},std={:.2f}\\nkurtosis={:.2f},outliers={:.2f},max={:.2f}\\nskew={:.2f}'.format(n,df_o[n].min(),df_o[n].mean(),df_o[n].std(),df_o[n].kurtosis(),upper,df_o[n].max(),df_o[n].skew()))    \n",
    "    axs[idx,3].set_title(info_o)\n",
    "\n",
    "    df_o[n].hist(ax=axs[idx,3])\n",
    "\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "df_log[outliers_tohandle]=df_o[outliers_tohandle]\n",
    "\n",
    "\n",
    "df_opt=df[df.columns[~df.columns.isin(non_categoric)]].join(df_log)\n",
    "\n",
    "# toignore=problematic+non_categoric\n",
    "# print(toignore)#=problematic+non_categoric\n",
    "\n",
    "# df_logs[problematic]=df[problematic]\n",
    "\n",
    "# df_opt=df[df.columns[~df.columns.isin(toignore)]].join(df_logs)\n",
    "\n",
    "\n",
    "df_opt.drop('pdays',axis=1,inplace=True)\n",
    "non_categoric.remove('pdays')\n",
    "\n",
    "print('transformed and scaled statistics of features/n',df_opt[non_categoric].describe())\n",
    "\n",
    "# ################tc whether to do this step now or after log ????????????????????????????\n",
    "# train=df_opt.loc[df_train.idx.tolist()]\n",
    "# test=df_opt.loc[df_test.idx.tolist()]\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.utils.validation import check_is_fitted, check_consistent_length\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.utils import check_matplotlib_support\n",
    "from sklearn.base import is_classifier\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    plot_confusion_matrix,\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    log_loss,\n",
    "    roc_auc_score,\n",
    "    SCORERS\n",
    ")\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression #baseline for binary classification problems\n",
    "from sklearn.ensemble import RandomForestClassifier # weak learners \n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "import time\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "\n",
    "train=df_opt.loc[df_train.idx.tolist()]\n",
    "test=df_opt.loc[df_test.idx.tolist()]\n",
    "\n",
    "X_train=train[train.columns[~train.columns.isin(['y', 'y_equiv','duration'])]].values\n",
    "y_train=train.loc[:,(train.columns =='y_equiv')].y_equiv.astype('int32')\n",
    "\n",
    "X_test=test[test.columns[~test.columns.isin(['y', 'y_equiv','duration'])]].values\n",
    "y_test=test.loc[:,(test.columns =='y_equiv')].y_equiv.astype('int32')\n",
    "\n",
    "scores = defaultdict(list)\n",
    "\n",
    "\n",
    "## logistic regression --> scale non-categoric with minmax since non is gaussian except duration which isn't contextually correct to use anyway\n",
    "#scaling necessary because of penality L2 that normalizes across rows not columns so they need to be relatable \n",
    "# non_categoric.remove('duration')\n",
    "df_lr=df_opt\n",
    "# for i in non_categoric:\n",
    "#     minmax=MinMaxScaler()\n",
    "#     minmaxfit=minmax.fit(train[[i]])\n",
    "#     tmp=minmax.transform(df_opt[i].values.reshape(-1, 1))\n",
    "# #    info=str('{}\\nmin={:.2f},mean={:.2f},std={:.2f}\\nkurtosis={:.2f},outliers={:.2f},max={:.2f}\\nskew={:.2f}'.format(i,df_opt[i].min(),df_opt[i].mean(),df_opt[i].std(),df_opt[i].kurtosis(),df_opt[i].quantile(.99),df_opt[i].max(),df_opt[i].skew()))      \n",
    "# #     print(info,'\\n')\n",
    "#     df_lr[i]=tmp\n",
    "features=df_lr.columns[~df_lr.columns.isin(['y', 'y_equiv'])]\n",
    "for i in features.tolist():\n",
    "    minmax=MinMaxScaler()\n",
    "    minmaxfit=minmax.fit(train[[i]].values)\n",
    "    tmp=minmax.transform(df_opt[i].values.reshape(-1, 1))\n",
    "#    info=str('{}\\nmin={:.2f},mean={:.2f},std={:.2f}\\nkurtosis={:.2f},outliers={:.2f},max={:.2f}\\nskew={:.2f}'.format(i,df_opt[i].min(),df_opt[i].mean(),df_opt[i].std(),df_opt[i].kurtosis(),df_opt[i].quantile(.99),df_opt[i].max(),df_opt[i].skew()))      \n",
    "#     print(info,'\\n')\n",
    "    df_lr[i]=tmp\n",
    "\n",
    "    \n",
    "train_lr=df_lr.loc[df_train.idx.tolist()]\n",
    "test_lr=df_lr.loc[df_test.idx.tolist()]\n",
    "\n",
    "X_train_lr=train_lr[train_lr.columns[~train_lr.columns.isin(['y', 'y_equiv'])]].values\n",
    "y_train_lr=train_lr.loc[:,(train_lr.columns =='y_equiv')].y_equiv.astype('int32')\n",
    "\n",
    "X_test_lr=test_lr[test_lr.columns[~test_lr.columns.isin(['y', 'y_equiv'])]].values\n",
    "y_test_lr=test_lr.loc[:,(test_lr.columns =='y_equiv')].y_equiv.astype('int32')\n",
    "###############################################################\n",
    "\n",
    "n_jobs = -1\n",
    "random_state=11\n",
    "\n",
    "clf=LogisticRegression(random_state=random_state, n_jobs=n_jobs, max_iter=4000)\n",
    "pipe=Pipeline(steps=[('lr',clf)])\n",
    "\n",
    "cv= RepeatedStratifiedKFold(n_splits=6, n_repeats=3, random_state=random_state)\n",
    "#cv = StratifiedKFold(shuffle=True, n_splits=6, random_state=random_state)\n",
    "\n",
    "\n",
    "# LogisticRegression().get_params().keys()\n",
    "param_grid = [{\n",
    "                'lr__penalty': ['l2','none'],\n",
    "                'lr__class_weight': ['balanced'],\n",
    "                'lr__solver': ['sag','newton-cg'],\n",
    "                'lr__multi_class': ['multinomial','ovr'],\n",
    "}]\n",
    "\n",
    "\n",
    "gs=GridSearchCV(pipe, param_grid, scoring='precision',cv=cv)\n",
    "gs.fit(X_train_lr, y_train_lr)\n",
    "\n",
    "print('best parameters are',gs.best_params_)\n",
    "start = time.time()\n",
    "\n",
    "y_predict=gs.predict(X_test_lr)\n",
    "y_prob = gs.predict_proba(X_test_lr) #model.predict_proba(X_test)[:,1]\n",
    "\n",
    "\n",
    "scores[\"Classifier\"].append('lr')\n",
    "\n",
    "for metric in [log_loss]:\n",
    "    score_name = metric.__name__.replace(\"_\", \" \").replace(\"score\", \"\").capitalize()\n",
    "    scores[score_name].append(metric(y_test_lr, y_prob[:, 1]))\n",
    "\n",
    "\n",
    "for metric in [accuracy_score,precision_score, recall_score, f1_score, roc_auc_score,r2_score]:#,calibration_curve_ece]:\n",
    "    score_name = metric.__name__.replace(\"_\", \" \").replace(\"score\", \"\").capitalize()\n",
    "    scores[score_name].append(metric(y_test_lr, y_predict))\n",
    "\n",
    "scores\n",
    "end = time.time()\n",
    "scores['time'].append(end-start)\n",
    "scores['bestmodel'].append(gs.best_params_)\n",
    "\n",
    "\n",
    "score_df = pd.DataFrame(scores).set_index(\"Classifier\")\n",
    "score_df.round(decimals=3)\n",
    "\n",
    "score_df  \n",
    "#y_prob[:,1]=y_predict\n",
    "\n",
    "## logisitc regression train w/o missing entries , expected to improve \n",
    "\n",
    "cols=['contact','poutcome','job','education','euribor3m']\n",
    "\n",
    "X_miss=train#_lr\n",
    "locs=[]\n",
    "\n",
    "for c in cols:\n",
    "    if c=='euribor3m':\n",
    "        cmiss=c+'_ffill'\n",
    "    else:\n",
    "        cmiss=c+'_missing'\n",
    "    tmp=X_miss.loc[(X_miss[c]==0) & (X_miss[cmiss]==1)].index.tolist()\n",
    "    \n",
    "\n",
    "    locs.append(tmp)\n",
    "\n",
    "toskiptrain=set(sorted(sum(locs, [])))\n",
    "\n",
    "print('{} samples from {} training samples have no-missing data'.format(-1*(len(toskiptrain)-len(train_lr)),len(train_lr)))\n",
    "\n",
    "\n",
    "train_clipped=train_lr.drop(toskiptrain,axis=0)\n",
    "train_clipped\n",
    "\n",
    "X_train_clipped=train_clipped[train_clipped.columns[~train_clipped.columns.isin(['y', 'y_equiv'])]].values\n",
    "y_train_clipped=train_clipped.loc[:,(train_clipped.columns =='y_equiv')].y_equiv.astype('int32')\n",
    "\n",
    "# clf2=gs.best_estimator_['lr']\n",
    "# # pipe=Pipeline(StandardScaler(),clf2)\n",
    "\n",
    "# pipe=make_pipeline(clf2)\n",
    "# pipe.fit(X_train_clipped, y_train_clipped)  # apply scaling on training data\n",
    "########################################################################################3\n",
    "\n",
    "clf=LogisticRegression(random_state=random_state, n_jobs=n_jobs, max_iter=4000)\n",
    "pipe=Pipeline(steps=[('lr',clf)])\n",
    "\n",
    "cv= RepeatedStratifiedKFold(n_splits=6, n_repeats=3, random_state=random_state)\n",
    "#cv = StratifiedKFold(shuffle=True, n_splits=6, random_state=random_state)\n",
    "\n",
    "# LogisticRegression().get_params().keys()\n",
    "param_grid = [{\n",
    "                'lr__penalty': ['l2','none'],\n",
    "                'lr__class_weight': ['balanced'],\n",
    "                'lr__solver': ['sag','newton-cg'],\n",
    "                'lr__multi_class': ['multinomial','ovr'],\n",
    "}]\n",
    "\n",
    "\n",
    "gs=GridSearchCV(pipe, param_grid, scoring='precision',cv=cv)\n",
    "gs.fit(X_train_clipped, y_train_clipped)\n",
    "\n",
    "##########################################################################################\n",
    "\n",
    "\n",
    "\n",
    "print('best parameters are',gs.best_params_)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "y_predict=gs.predict(X_test_lr)\n",
    "\n",
    "y_prob = gs.predict_proba(X_test_lr)\n",
    "\n",
    "scores[\"Classifier\"].append('lr-missingvalues')\n",
    "\n",
    "for metric in [log_loss]:\n",
    "    score_name = metric.__name__.replace(\"_\", \" \").replace(\"score\", \"\").capitalize()\n",
    "    scores[score_name].append(metric(y_test_lr, y_prob[:, 1]))\n",
    "\n",
    "\n",
    "for metric in [accuracy_score,precision_score, recall_score, f1_score, roc_auc_score,r2_score]:#,calibration_curve_ece]:\n",
    "    score_name = metric.__name__.replace(\"_\", \" \").replace(\"score\", \"\").capitalize()\n",
    "    scores[score_name].append(metric(y_test_lr, y_predict))\n",
    "\n",
    "end = time.time()\n",
    "scores['time'].append(end-start)\n",
    "scores['bestmodel'].append(gs.best_params_)\n",
    "\n",
    "score_df = pd.DataFrame(scores).set_index(\"Classifier\")\n",
    "score_df.round(decimals=3)\n",
    "\n",
    "score_df  \n",
    "\n",
    "##random forest \n",
    "\n",
    "n_jobs = -1\n",
    "random_state=11\n",
    "\n",
    "clf=RandomForestClassifier(n_jobs=n_jobs,random_state=random_state)#20 is nothing, std is 100,min_samples_split 5? default=2,min_samples_leafint 3? default=1 \n",
    "\n",
    "pipe=Pipeline(steps=[('rf',clf)])\n",
    "\n",
    "cv= RepeatedStratifiedKFold(n_splits=6, n_repeats=3, random_state=random_state)\n",
    "#cv = StratifiedKFold(shuffle=True, n_splits=6, random_state=random_state)\n",
    "\n",
    "## ince you are using scikit, try gradient boosted trees on your data. You'll probably get better precision-recall AUC right out of the box. SVCs, as you point out, are not really practical for anything but very small datasets.\n",
    "\n",
    "# LogisticRegression().get_params().keys()\n",
    "param_grid = [{\n",
    "                'rf__n_estimators': [150],\n",
    "                'rf__max_depth':[len(X_train/2),3*len(X_train)/4],\n",
    "                'rf__criterion': ['gini','entropy'],\n",
    "                'rf__class_weight': ['balanced', 'balanced_subsample'],\n",
    "                'rf__oob_score':[True],  \n",
    "                'rf__max_samples':[0.2,0.4],\n",
    "}]\n",
    "\n",
    "\n",
    "gs=GridSearchCV(pipe, param_grid, scoring='precision',cv=cv)\n",
    "\n",
    "gs.fit(X_train, y_train)\n",
    "\n",
    "print('best parameters are',gs.best_params_)\n",
    "start = time.time()\n",
    "\n",
    "y_predict=gs.predict(X_test)\n",
    "y_prob = gs.predict_proba(X_test) #model.predict_proba(X_test)[:,1]\n",
    "\n",
    "#scores = defaultdict(list)\n",
    "\n",
    "scores[\"Classifier\"].append('rf')\n",
    "\n",
    "for metric in [log_loss]:\n",
    "    score_name = metric.__name__.replace(\"_\", \" \").replace(\"score\", \"\").capitalize()\n",
    "    scores[score_name].append(metric(y_test, y_prob[:, 1]))\n",
    "\n",
    "\n",
    "for metric in [accuracy_score,precision_score, recall_score, f1_score, roc_auc_score,r2_score]:#,calibration_curve_ece]:\n",
    "    score_name = metric.__name__.replace(\"_\", \" \").replace(\"score\", \"\").capitalize()\n",
    "    scores[score_name].append(metric(y_test, y_predict))\n",
    "\n",
    "end = time.time()\n",
    "scores['time'].append(end-start)\n",
    "scores['bestmodel'].append(gs.best_params_)\n",
    "\n",
    "score_df = pd.DataFrame(scores).set_index(\"Classifier\")\n",
    "score_df.round(decimals=3)\n",
    "gs_rf=gs\n",
    "score_df  \n",
    "\n",
    "## bagging classifier\n",
    "\n",
    "n_jobs = -1\n",
    "random_state=11\n",
    "from imblearn.ensemble import BalancedBaggingClassifier \n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "                               \n",
    "clf3=gs.best_estimator_['rf']\n",
    "# clf3=score_df.loc['rf'].bestmodel\n",
    "bag=BalancedBaggingClassifier(base_estimator=clf3,replacement=False,\n",
    "                                random_state=random_state,\n",
    "                             n_jobs=n_jobs)\n",
    "\n",
    "pipe=Pipeline(steps=[('bag',bag)])\n",
    "\n",
    "cv= RepeatedStratifiedKFold(n_splits=6, n_repeats=3, random_state=random_state)\n",
    "# cv = StratifiedKFold(shuffle=True, n_splits=6, random_state=random_state)\n",
    "\n",
    "## ince you are using scikit, try gradient boosted trees on your data. You'll probably get better precision-recall AUC\n",
    "#right out of the box. SVCs, as you point out, are not really practical for anything but very small datasets.\n",
    "\n",
    "# LogisticRegression().get_params().keys()\n",
    "param_grid = [{\n",
    "                'bag__sampling_strategy':['all','not majority','majority'],\n",
    "}]\n",
    "\n",
    "gs=GridSearchCV(pipe, param_grid, scoring='precision',cv=cv)\n",
    "\n",
    "gs.fit(X_train, y_train)\n",
    "\n",
    "print('best parameters are',gs.best_params_)\n",
    "start = time.time()\n",
    "y_predict=gs.predict(X_test)\n",
    "y_prob = gs.predict_proba(X_test) #model.predict_proba(X_test)[:,1]\n",
    "\n",
    "#scores = defaultdict(list)\n",
    "\n",
    "scores[\"Classifier\"].append('bag_rf')\n",
    "\n",
    "for metric in [log_loss]:\n",
    "    score_name = metric.__name__.replace(\"_\", \" \").replace(\"score\", \"\").capitalize()\n",
    "    scores[score_name].append(metric(y_test, y_prob[:, 1]))\n",
    "\n",
    "\n",
    "for metric in [accuracy_score,precision_score, recall_score, f1_score, roc_auc_score,r2_score]:#,calibration_curve_ece]:\n",
    "    score_name = metric.__name__.replace(\"_\", \" \").replace(\"score\", \"\").capitalize()\n",
    "    scores[score_name].append(metric(y_test, y_predict))\n",
    "\n",
    "end = time.time()\n",
    "scores['time'].append(end-start)\n",
    "scores['bestmodel'].append(gs.best_params_)\n",
    "\n",
    "score_df = pd.DataFrame(scores).set_index(\"Classifier\")\n",
    "score_df.round(decimals=3)\n",
    "\n",
    "score_df     \n",
    "\n",
    "n_jobs = -1\n",
    "random_state=11\n",
    "\n",
    "clf=RandomForestClassifier(n_jobs=n_jobs,random_state=random_state)#20 is nothing, std is 100,min_samples_split 5? default=2,min_samples_leafint 3? default=1 \n",
    "\n",
    "pipe=Pipeline(steps=[('rf',clf)])\n",
    "\n",
    "cv= RepeatedStratifiedKFold(n_splits=6, n_repeats=3, random_state=random_state)\n",
    "# cv = StratifiedKFold(shuffle=True, n_splits=6, random_state=random_state)\n",
    "\n",
    "## ince you are using scikit, try gradient boosted trees on your data. You'll probably get better precision-recall AUC right out of the box. SVCs, as you point out, are not really practical for anything but very small datasets.\n",
    "train_clipped=train.drop(toskiptrain,axis=0)\n",
    "train_clipped\n",
    "\n",
    "X_train_clipped=train_clipped[train_clipped.columns[~train_clipped.columns.isin(['y', 'y_equiv'])]].values\n",
    "y_train_clipped=train_clipped.loc[:,(train_clipped.columns =='y_equiv')].y_equiv.astype('int32')\n",
    "\n",
    "# LogisticRegression().get_params().keys()\n",
    "param_grid = [{\n",
    "                'rf__n_estimators': [150],\n",
    "                'rf__max_depth':[len(X_train_clipped)/2,3*len(X_train_clipped)/4],\n",
    "                'rf__criterion': ['gini','entropy'],\n",
    "                'rf__class_weight': ['balanced', 'balanced_subsample'],\n",
    "                'rf__oob_score':[True],  \n",
    "                'rf__max_samples':[0.2,0.4],\n",
    "}]\n",
    "\n",
    "\n",
    "gs=GridSearchCV(pipe, param_grid, scoring='precision',cv=cv)\n",
    "\n",
    "gs.fit(X_train_clipped, y_train_clipped)\n",
    "\n",
    "print('best parameters are',gs.best_params_)\n",
    "start = time.time()\n",
    "\n",
    "y_predict=gs.predict(X_test)\n",
    "y_prob = gs.predict_proba(X_test) #model.predict_proba(X_test)[:,1]\n",
    "\n",
    "#scores = defaultdict(list)\n",
    "\n",
    "scores[\"Classifier\"].append('rf_missingvalues')\n",
    "\n",
    "for metric in [log_loss]:\n",
    "    score_name = metric.__name__.replace(\"_\", \" \").replace(\"score\", \"\").capitalize()\n",
    "    scores[score_name].append(metric(y_test, y_prob[:, 1]))\n",
    "\n",
    "\n",
    "for metric in [accuracy_score,precision_score, recall_score, f1_score, roc_auc_score,r2_score]:#,calibration_curve_ece]:\n",
    "    score_name = metric.__name__.replace(\"_\", \" \").replace(\"score\", \"\").capitalize()\n",
    "    scores[score_name].append(metric(y_test, y_predict))\n",
    "\n",
    "end = time.time()\n",
    "scores['time'].append(end-start)\n",
    "scores['bestmodel'].append(gs.best_params_)\n",
    "\n",
    "score_df = pd.DataFrame(scores).set_index(\"Classifier\")\n",
    "score_df.round(decimals=3)\n",
    "\n",
    "score_df\n",
    "\n",
    "from collections import Counter\n",
    "counter = Counter(y_train)\n",
    "display(counter)\n",
    "# estimate scale_pos_weight value\n",
    "#estimate = counter[0] / counter[1]\n",
    "#print('Estimate: %.3f' % estimate)\n",
    "\n",
    "for metric in [accuracy_score,precision_score, recall_score, f1_score, roc_auc_score,r2_score]:#,calibration_curve_ece]:\n",
    "    score_name = metric.__name__.replace(\"_\", \" \").replace(\"score\", \"\").capitalize()\n",
    "    print(score_name)\n",
    "    #scores[score_name].append(metric(y_test, y_predict))\n",
    "    \n",
    "##xgb boost \n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "counter = Counter(y_train)\n",
    "display(counter)\n",
    "# estimate scale_pos_weight value\n",
    "estimate = counter[0] / counter[1]\n",
    "print('Estimate: %.3f' % estimate)\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "# weights = [1, 10, 25, 50, 75, 99, 100, 1000]\n",
    "# param_grid = dict(scale_pos_weight=weights)\n",
    "\n",
    "n_jobs = -1\n",
    "random_state=11\n",
    "\n",
    "clf=XGBClassifier(n_jobs=n_jobs,random_state=random_state)#20 is nothing, std is 100,min_samples_split 5? default=2,min_samples_leafint 3? default=1 \n",
    "\n",
    "pipe=Pipeline(steps=[ ('xgb',clf)])\n",
    "\n",
    "cv= RepeatedStratifiedKFold(n_splits=6, n_repeats=3, random_state=random_state)\n",
    "# cv = StratifiedKFold(shuffle=True, n_splits=6, random_state=random_state)\n",
    "\n",
    "## ince you are using scikit, try gradient boosted trees on your data. You'll probably get better precision-recall AUC right out of the box. SVCs, as you point out, are not really practical for anything but very small datasets.\n",
    "\n",
    "# LogisticRegression().get_params().keys()\n",
    "param_grid = [{\n",
    "    'xgb__scale_pos_weight': [2/estimate,3/estimate,5/estimate,estimate**(1./5),estimate**(1./3),estimate**(1./2),estimate]\n",
    "}]\n",
    "\n",
    "\n",
    "gs=GridSearchCV(pipe, param_grid, scoring='precision',cv=cv)\n",
    "\n",
    "gs.fit(X_train, y_train)\n",
    "\n",
    "print('best parameters are',gs.best_params_)\n",
    "start = time.time()\n",
    "\n",
    "y_predict=gs.predict(X_test)\n",
    "y_prob = gs.predict_proba(X_test) #model.predict_proba(X_test)[:,1]\n",
    "\n",
    "#scores = defaultdict(list)\n",
    "\n",
    "scores[\"Classifier\"].append('xgb')\n",
    "\n",
    "for metric in [log_loss]:\n",
    "    score_name = metric.__name__.replace(\"_\", \" \").replace(\"score\", \"\").capitalize()\n",
    "    scores[score_name].append(metric(y_test, y_prob[:, 1]))\n",
    "\n",
    "\n",
    "for metric in [accuracy_score,precision_score, recall_score, f1_score, roc_auc_score,r2_score]:#,calibration_curve_ece]:\n",
    "    score_name = metric.__name__.replace(\"_\", \" \").replace(\"score\", \"\").capitalize()\n",
    "    scores[score_name].append(metric(y_test, y_predict))\n",
    "\n",
    "end = time.time()\n",
    "scores['time'].append(end-start)\n",
    "scores['bestmodel'].append(gs.best_params_)\n",
    "\n",
    "score_df = pd.DataFrame(scores).set_index(\"Classifier\")\n",
    "score_df.round(decimals=3)\n",
    "\n",
    "score_df  \n",
    "\n",
    "score_df.to_csv(\"final_models_results_duration.csv\",\n",
    "          index=False,\n",
    "          encoding=\"utf-8\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
